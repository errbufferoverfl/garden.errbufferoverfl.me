---
title: March 2021 Submission in Response to the Online Saftey Bill
subtitle: Submission to the Senate Environment and Communications Legislation Committee
author:
  - errbufferoverfl
date: 2021-03-01T22:20:25+10:30
date-modified: 2024-10-12T09:21:49+10:30
description: My March 2021 submission provided to the Senate Environment and Communications Legislation Committee regarding the Online Safety Bill 2021. The submission raises concerns about the bill's impact on marginalized communities, especially sex workers, and critiques the broad powers granted to the eSafety Commissioner. It calls for transparency, oversight, and community representation in the bill’s implementation, emphasizing the need for updates to the National Classification Code and protections for online content creators.
categories:
  - Government Submission
  - Online Saftey Bill
---

**Online Safety Bill – Submission to the Senate Environment and Communications Legislation Committee**

**RE: Consultation on a new Online Safety Act**

To Whom It May Concern: Thank you for the opportunity to provide this submission to the Senate Environment and Communications Legislation Committee inquiry into the Online Safety Bill 2021.

During the original consultation period I provided a submission (Appendix A) expressing my concerns about the bill and their impact to Netizens and already marginalised communities including sex workers. However, I would also like to highlight some additional areas of concern:

The eSaftey Commissioner has publicly stated that removal of child sexual abuse material remains the eSafety’s priority – however, it is important to realise this class of material is already illegal, which begs the question of why the Commissioner seeks to extend these powers to also monitor X18+ and RC content.

Part 4 of the Online Safety Bill gives the eSaftey Commissioner power to determine “basic online safety” expectation for social media, and other relevant electronic services with the aim to “Minimise cyber-bullying or abuse material targeted at a child or adult, non-consensual intimate images, Class 1 material and abhorrent violent material”.

This concern extends to the issue that the expectations are too broad and is likely to result in the excessive monitoring and removal of X18+ and RC content. Furthermore, the current National Classification Code (NCC) is outdated and was accepting submissions at the same time as the Online Safety Bill.

Considering the NCC has only just undergone review, it is not appropriate that the eSaftey Commissioner has broad discretion to determine basic online safety expectations until the NCC has been revised and updated.

In addition, there is concern that because the eSaftey Commissioner has broad discretion to determine what is in the public interest – this may result in unforeseen circumstances such as moral policing and the removal of media that depicts violence that should become public knowledge – for example the viral video of an NSW Police Officer using excessive force against an Aboriginal teenager.

Finally, the bill currently relies heavily on the NCC to determine which content should be issued with a removal notice – as previously stated the NCC in its current form is outdated and does not reflect a modern classification system, with Class 1 and 2 encapsulating all sexual content, violent or not. This undermines the livelihood of sex workers many of whom have leveraged online services to continue to work during the COVID-19 pandemic and may force many offline into unsafe working environments.

Ultimately, I have many concerns the bill as currently drafted is inadequate and uses an outdated approach to ensure the safety of netizens. The powers granted to the eSaftey Commissioner are too broad with no transparency to how this department operates and to what extent they are investigating and using these powers.

The bill should be amended to include provisions around:

- A multi-stakeholder oversight panel made up of broad community representation should be put together and review decisions about the removal and blocking of content;
- Enforced public reporting of the categories of content take-downs, complaints and blocking notices issued, including reasoning. This allows the public and Parliamentary scrutiny over the scope and impact of the Bill.
- An internal review process, so people can challenge removal notices without having to appeal to the Administrative Appeals Tribunal.
- Further community review should be conducted with broad community representation – including organisations such as Scarlett Alliance, Assembly Four, Digital Rights Watch, the Electronic Frontiers Australia.

## Appendix A

While it is positive to see the bill addresses the unaddressed power technology companies have in remaining complicit in the distribution of non-consensual content online, it is concerning to see that the bill does not address the lack of transparency and unaccountable approach that the E-Safety Commissioner does and will continue to take.

**Issue One: Operations of the E-Safety Commissioner**

The primary concern is in its current form the E-Safety Commissioner is a single unelected official who can make decisions about online content with very little oversight, transparency or consequence. This puts Internet consumers at the mercy of the biases and beliefs of this official.

An example of this is our current commissioner Julie Inman Grant who was appointed in 2017, Grant also sits on the board of WeProtect Global Alliance,[^1] a non-profit organisation whose focus is in decreasing the proliferation and accessibility of child abuse images. However, the birth of the WeProtect came from Prime Minister David Cameron announcement in July, 2013[^2] the new UK-US taskforce the focus was not only in making the Internet a safer place for children but also cracking down on online pornography.

A non-for-profit whose origins lie in the management of child safety and the reduction of online pornography runs the risk of conflating these two issues and imposing moral values not shared by the wider community. That is to say that decreasing the proliferation and accessibility of child abuse images should be the sole goal.

As part of sitting on this board our current commissioner risks introducing these biases into the very system that aims to protect young children. Further adding to this point is that the current government website for the E-Safety Commissioner lists “sexually explicit”[^3] material along-side other prohibited material such as:

- matters of crime or violence
- instruction in paedophilia
- advocates terrorist acts
- depicts gratuitous depictions of violence and sexual violence

The concern here is that “is sexually explicit” is incredibly vague and does not distinguish between material that is sexually explicit and legal, and sexually explicit and illegal. Leaving a lot of room for subjectivity and interpretation which is not an appropriate framework for making decisions about content regulation.

If the commissioner, who as previously mentioned, is not elected and faces no real transparency requirements deems the matter requires investigation, the commissioner can take the following action:

- Issue a removal notice requiring the content to be removed.
- Issue a remedial notice requiring the content to be subject to a ‘restricted access system’ or to stop hosting the material.
- Issue a link deletion notice requiring any links to the content to be deleted, or-issue an app deletion notice requiring services to stop enabling Australian users from downloading an app.

This continues to highlight the concern that legal hosting of sexually explicit material is removed or punished because the commission determines it goes against their own moral values rather than poses a real harm.

Furthermore, the commissioner is not required to justify their reason to pick to investigate some reports and not others. While the official website outlines that there is a priority given to reports of child sex abuse reports,[^4] there is no requirement for statistics on enforcement and compliance patterns to be published.

While reporting statistics are provided in the Australian Communications and Media Authority & Office of the E-Safety Commissioner Annual Report[^5] there is no mention of enforcement and compliance patterns. Therefore, the public will not know how many complaints have been made against various people, services, or companies or why some content is subject to some actions and not others. Consequently, there is no way for the public to determine how much harm is being prevented by the commissioner.

Additionally, with the list of actions the commissioner can take, there is no ability for a user to respond to (re: edit) their content to ensure that it complies with the proposed framework, as there is no clear definition of what is harmful.

As it currently stands sexually explicit material is considered “RC” if it contains violence, sexualised violence, coercion, sexually assaultive language, fetishes, or depictions which purposefully demean anyone, even if it is between consenting adults. This results in a narrow definition of what is considered harmful and by default assumes – as demonstrated by the groupings – fetish material is considered dangerous, with the potential inclusion of sexual acts such as rough sex (described as depictions which purposefully demean anyone) and dirty talk (described as sexually assaultive language).

The National Classification Scheme is currently being reviewed with submissions closing on February, 19 2020[^6] - the results of which are still inconclusive, and the government is yet to come up with a way to refresh the system, therefore we should not be reproducing and making this classification system a dependency in which we base the proposed Online Safety Act. The government should allow the people of Australia to have their say on the the National Classification Scheme and then address the Online Safety Act.

 **Issue 2: Non-Consensual Sharing of Intimate Images**

A positive portion of the bill is that it creates a system for people depicted in intimate images to have recourse and can complain about their images being posted online. These images which include pictures of the genitals, anus or breasts, intimate activities (such as undressing or sexual acts) where there is a reasonable expectation of privacy.

This portion although positive is not equitable as stated in part one of this response, the E-Safety Commissioner holds power of who and what to investigate and issuing notice. If the E-Safety Commissioner holds a bias towards a reporter, such as a sex worker who has had intimate pictures taken without their consent, they may opt to not investigate.

Which continues to demonstrate the importance of oversight and accountability to ensure all complaints are handled fairly and equitable, regardless of personal beliefs, biases or stigma. If the commissioner cannot act fairly in favour of some, then the commissioner cannot act fairly for all.

Additionally, the existing section does not recognise the withdrawal of consent and limits on consent. A person may consent to have their intimate images posted online for a particular purpose, for example advertising on an escorting website, as part of an art installation, for personal usage) however, may not consent for the publication on other purposes.

The bill should be amended to recognise that people may consent to the collection and dissemination of intimate images for one purpose but this does not permit the collector to use or disseminate the images for another reason. Furthermore, the bills needs to include provisions that recognise people may withdraw their consent for the posting of intimate images at any time.

Finally, the government and E-Safety Commissioner should be engaging with sex workers, businesses and technology providers to ensure that people who do sell sex work, intimate images or other explicit material are included and consulted as part of legislation, bills and other initiatives.

Many sex workers already work to prevent minors from accessing inappropriate material through paywalls, 18+ warnings and age verification processes – however are still often treated as a problem or liability for online safety.

The reality is, by trying to push sex workers out of the discussion and prevent them from using, advertising, or selling services online, there is a high likelihood of pushing sex workers further underground which introduces risk that workers will be further harmed, whether online, in-person, or through systematic structures the government allows to be built.

Thank you for taking the time to read this submission.

Originally published as [submission 111](https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Environment_and_Communications/OnlineSafety/Submissions).

[^1]: [https://www.weprotect.org/alliance/governance/board-members/](https://www.weprotect.org/alliance/governance/board-members/)
[^2]: [https://www.gov.uk/government/speeches/the-internet-and-pornography-prime-minister-calls-for-action](https://www.gov.uk/government/speeches/the-internet-and-pornography-prime-minister-calls-for-action)
[^3]:[https://www.esafety.gov.au/report/illegal-harmful-content](https://www.esafety.gov.au/report/illegal-harmful-content)
[^4]:[https://www.esafety.gov.au/report/illegal-harmful-content/what-we-can-investigate](https://www.esafety.gov.au/report/illegal-harmful-content/what-we-can-investigate)
[^5]:[https://www.acma.gov.au/sites/default/files/2019-06/ACMA_OeSC-annual-reports-2017-18-pdf.pdf](https://www.acma.gov.au/sites/default/files/2019-06/ACMA_OeSC-annual-reports-2017-18-pdf.pdf)
[^6]:[https://www.communications.gov.au/have-your-say/review-australian-classification-regulation](https://www.communications.gov.au/have-your-say/review-australian-classification-regulation)
